{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "''' TESTING CELL\n",
        "Run this cell to test the trained model on test set.\n",
        "Instructions to Run:\n",
        "    1. Uncomment the drive.mount line to load dataset from drive\n",
        "    2. Define test_loader to the test set in the inference loop.\n",
        "    3. Define criterion function to get the result metrics or comment out the line in the inference loop.\n",
        "    4. Replace the paths to the generator.pt and model.pt\n",
        "    5. Run the cell to obtain the test results\n",
        "'''\n",
        "\n",
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Mount Google Drive if needed\n",
        "# drive.mount('/content/drive') # Uncomment if loading from Google Drive\n",
        "\n",
        "# define transformations\n",
        "transform = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ])\n",
        "\n",
        "# Dataset class for paired original and corrupted images\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root_dir_original, root_dir_corrupted, transform=None):\n",
        "        self.root_dir_original = root_dir_original\n",
        "        self.root_dir_corrupted = root_dir_corrupted\n",
        "        self.transform = transform\n",
        "\n",
        "        self.original_image_list = os.listdir(root_dir_original)\n",
        "        self.corrupted_image_list = os.listdir(root_dir_corrupted)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.original_image_list), len(self.corrupted_image_list))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_img_name = self.original_image_list[idx]\n",
        "        corrupted_img_name = self.corrupted_image_list[idx]\n",
        "\n",
        "        original_img_path = os.path.join(self.root_dir_original, original_img_name)\n",
        "        corrupted_img_path = os.path.join(self.root_dir_corrupted, corrupted_img_name)\n",
        "\n",
        "        # Load the original and corrupted images\n",
        "        original_img = Image.open(original_img_path).convert('RGB')\n",
        "        corrupted_img = Image.open(corrupted_img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            original_img = self.transform(original_img)\n",
        "            corrupted_img = self.transform(corrupted_img)\n",
        "\n",
        "        return original_img, corrupted_img, corrupted_img_name\n",
        "\n",
        "# Generator class for image inpainting\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self,num_channels=3,latent_dim=100):\n",
        "        super(Generator,self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.fc = nn.Linear(512 * 16 * 16, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512 * 16 * 16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 16, 16)),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ConvTranspose2d(64, num_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# BasicBlock class for ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet class for image classification\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Function to create a ResNet-50 model\n",
        "def resnet50():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "# Instantiate models for inpainting and classification\n",
        "model_inpainting = Generator()\n",
        "g = torch.load('/content/drive/MyDrive/generator.pt') # Replace with path to generator.pt\n",
        "model_inpainting.load_state_dict(g)\n",
        "\n",
        "model_classification = resnet50()\n",
        "m = torch.load('/content/drive/MyDrive/model.pt') #Replace with path to model.pt\n",
        "model_classification.load_state_dict(m)\n",
        "\n",
        "# Set models to evaluation mode\n",
        "model_inpainting.eval()\n",
        "model_classification.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to CPU or GPU\n",
        "model_inpainting.to(device)\n",
        "model_classification.to(device)\n",
        "\n",
        "p = torch.tensor([])\n",
        "t = torch.tensor([])\n",
        "\n",
        "# Test the models on the test_loader\n",
        "for data, target in test_loader:\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Generate inpainted image\n",
        "    inpainted_img_tensor = model_inpainting(data)\n",
        "\n",
        "    # Classify the inpainted image\n",
        "    output = model_classification(inpainted_img_tensor)\n",
        "\n",
        "    # Calculate and print loss\n",
        "    loss = criterion(output, target)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # t=torch.cat((t,target.cpu()))\n",
        "    # p=torch.cat((p,pred.cpu()))\n",
        "    # acc = accuracy_score(target.cpu(),pred.cpu())\n",
        "    # print(f'pred: {pred}\\n target: {target} \\n {acc}')"
      ],
      "metadata": {
        "id": "qxegOA4ncrml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia8JdoVJWy22"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/train+val.zip'\n",
        "!mkdir train/all\n",
        "!mkdir train/all_paired\n",
        "!cp train/Flood/* train/all\n",
        "!cp train/Hurricane/* train/all\n",
        "!cp train/Earthquake/* train/all\n",
        "!cp train/Landslides/* train/all\n",
        "!cp train/Wildfire/* train/all\n",
        "!cp train/Earthquake_paired/* train/all_paired\n",
        "!cp train/Flood_paired/* train/all_paired\n",
        "!cp train/Hurricane_paired/* train/all_paired\n",
        "!cp train/Landslides_paired/* train/all_paired\n",
        "!cp train/Wildfire_paired/* train/all_paired\n",
        "!rm train/all/*.txt\n",
        "!rm train/all_paired/*.txt"
      ],
      "metadata": {
        "id": "me4a-czRYCaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root_dir_original, root_dir_corrupted, transform=None):\n",
        "        self.root_dir_original = root_dir_original\n",
        "        self.root_dir_corrupted = root_dir_corrupted\n",
        "        self.transform = transform\n",
        "\n",
        "        self.original_image_list = os.listdir(root_dir_original)\n",
        "        self.corrupted_image_list = os.listdir(root_dir_corrupted)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.original_image_list), len(self.corrupted_image_list))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_img_name = self.original_image_list[idx]\n",
        "        corrupted_img_name = self.corrupted_image_list[idx]\n",
        "\n",
        "        original_img_path = os.path.join(self.root_dir_original, original_img_name)\n",
        "        corrupted_img_path = os.path.join(self.root_dir_corrupted, corrupted_img_name)\n",
        "\n",
        "        # Load the original and corrupted images\n",
        "        original_img = Image.open(original_img_path).convert('RGB')\n",
        "        corrupted_img = Image.open(corrupted_img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            original_img = self.transform(original_img)\n",
        "            corrupted_img = self.transform(corrupted_img)\n",
        "\n",
        "        return original_img, corrupted_img, corrupted_img_name"
      ],
      "metadata": {
        "id": "eGWXsF_EY36K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your datasets\n",
        "original_dataset_root = '/content/train/all'\n",
        "corrupted_dataset_root = '/content/train/all_paired'\n",
        "transform = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                ])\n",
        "# Create the dataset\n",
        "paired_image_dataset = PairedImageDataset(root_dir_original=original_dataset_root,\n",
        "                                          root_dir_corrupted=corrupted_dataset_root,\n",
        "                                          transform=transform)\n",
        "\n",
        "# Create the dataloader\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(paired_image_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "lnKThwEYZB-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,num_channels=3,latent_dim=100):\n",
        "        super(Generator,self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        self.fc = nn.Linear(512 * 16 * 16, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512 * 16 * 16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Unflatten(1, (512, 16, 16)),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ConvTranspose2d(64, num_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f2zI8IFQZrqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN-7O50CSD9S"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(num_channels * 2, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 32 * 32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rThGXrOWLYkL"
      },
      "outputs": [],
      "source": [
        "patch_size = 16\n",
        "class LocalDiscriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_patches=4):\n",
        "        super(LocalDiscriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels * 2, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*16*16,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * patch_size * patch_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whPmSt6ZzbUf"
      },
      "outputs": [],
      "source": [
        "patch_size = 64\n",
        "class LocalDiscriminator64(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_patches=4):\n",
        "        super(LocalDiscriminator64, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_channels * 2, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*8*8,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * patch_size * patch_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_channels=3\n",
        "def gan_loss(generator, discriminator, local_discriminator, original_images, corrupted_images, lambda_recon=100, lambda_global=100, lambda_local=100):\n",
        "    generator=generator.to(device)\n",
        "    discriminator=discriminator.to(device)\n",
        "    local_discriminator=local_discriminator.to(device)\n",
        "    original_images=original_images.to(device)\n",
        "    corrupted_images=corrupted_images.to(device)\n",
        "\n",
        "    # Forward pass through the generator\n",
        "    inpainted_images = generator(corrupted_images)\n",
        "\n",
        "    # Concatenate original and inpainted images along the channel dimension\n",
        "    combined_images = torch.cat((original_images, inpainted_images), dim=1)\n",
        "    combined_images=combined_images.to(device)\n",
        "\n",
        "    # Forward pass through the discriminator\n",
        "    discriminator_output = discriminator(combined_images)\n",
        "\n",
        "    # Target labels for the discriminator\n",
        "    real_labels = torch.ones_like(discriminator_output)\n",
        "    fake_labels = torch.zeros_like(discriminator_output)\n",
        "\n",
        "    # Reconstruction loss (L1 loss)\n",
        "    recon_loss = nn.functional.l1_loss(inpainted_images, original_images) * lambda_recon\n",
        "\n",
        "    # Global discriminator loss (binary cross-entropy)\n",
        "    global_discriminator_loss = F.binary_cross_entropy(discriminator_output, real_labels) * lambda_global\n",
        "\n",
        "    # Local discriminator loss (binary cross-entropy on local patches)\n",
        "    local_discriminator_loss = 0.0\n",
        "    patch_size = 64\n",
        "\n",
        "    for i in range(0, original_images.size(2) - patch_size + 1, patch_size):\n",
        "        for j in range(0, original_images.size(3) - patch_size + 1, patch_size):\n",
        "            # Extract local patches from original and inpainted images\n",
        "            original_patch = original_images[:, :, i:i+patch_size, j:j+patch_size]\n",
        "            inpainted_patch = inpainted_images[:, :, i:i+patch_size, j:j+patch_size]\n",
        "\n",
        "            # Concatenate patches along the channel dimension\n",
        "            combined_patch = torch.cat((original_patch, inpainted_patch), dim=1)\n",
        "            combined_patch = combined_patch.to(device)\n",
        "            # Forward pass through the local discriminator\n",
        "            discriminator_output_patch = local_discriminator(combined_patch)\n",
        "            # Compute local discriminator loss\n",
        "            local_discriminator_loss += F.binary_cross_entropy(discriminator_output_patch, real_labels) * lambda_local\n",
        "\n",
        "    # Total GAN loss\n",
        "    total_loss = recon_loss + global_discriminator_loss + local_discriminator_loss\n",
        "\n",
        "    return total_loss, recon_loss, global_discriminator_loss, local_discriminator_loss"
      ],
      "metadata": {
        "id": "Unxe3-DMaACC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "local_discriminator = LocalDiscriminator64()\n",
        "\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "local_discriminator = local_discriminator.to(device)\n",
        "\n",
        "lambda_recon = 10\n",
        "lambda_global = 1\n",
        "lambda_local = 1\n",
        "\n",
        "\n",
        "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.00001, betas=(0.5, 0.999))\n",
        "optimizer_local_discriminator = torch.optim.Adam(local_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "optimizer_generator.zero_grad()\n",
        "optimizer_discriminator.zero_grad()\n",
        "optimizer_local_discriminator.zero_grad()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (original_images, corrupted_images) in tqdm(enumerate(dataloader)):\n",
        "        # Set model gradients to zero\n",
        "\n",
        "        # Forward pass and calculate GAN loss\n",
        "        total_loss, recon_loss, global_discriminator_loss, local_discriminator_loss = gan_loss(generator, discriminator, local_discriminator, original_images, corrupted_images, lambda_recon, lambda_global, lambda_local)\n",
        "\n",
        "        # Backward pass and optimization steps\n",
        "        total_loss.backward()\n",
        "        optimizer_generator.step()\n",
        "        optimizer_discriminator.step()\n",
        "        optimizer_local_discriminator.step()\n",
        "        optimizer_generator.zero_grad()\n",
        "        optimizer_discriminator.zero_grad()\n",
        "        optimizer_local_discriminator.zero_grad()\n",
        "\n",
        "        # Print training statistics\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], recon Loss: {recon_loss.item():.6f}, GD Loss: {global_discriminator_loss.item():.6f}, Local Loss: {local_discriminator_loss:.6f}, Total Loss: {total_loss.item():.6f}')\n",
        "    if epoch%5==4:\n",
        "        torch.save(generator.state_dict(), '/content/drive/MyDrive/generator.pt')\n",
        "        torch.save(discriminator.state_dict(), '/content/drive/MyDrive/discriminator.pt')\n",
        "        torch.save(local_discriminator.state_dict(), '/content/drive/MyDrive/local_discriminator.pt')\n",
        "        print(f'Models saved successfully on epoch: {epoch+1}')"
      ],
      "metadata": {
        "id": "FUY_pIabaj1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder('/content/Train',transform=transform)\n",
        "validation_dataset = datasets.ImageFolder('/content/validation',transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "tD_Mmp5hbWMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def resnet50():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])"
      ],
      "metadata": {
        "id": "YanEaWchbWgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, optimizer, and loss function\n",
        "model = resnet50()\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.005,betas=(0.9,0.999))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "OZXrTFOLbWkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "num_epochs = 25\n",
        "model.eval()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    # Calculate training accuracy\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(validation_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "    if epoch%5==4:\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/model.pt')"
      ],
      "metadata": {
        "id": "mZM84E8EbWpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in validation_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        predictions = predictions.cpu()\n",
        "        all_predictions.extend(predictions.numpy())\n",
        "result = torch.tensor(all_predictions)"
      ],
      "metadata": {
        "id": "KaPVuRz9bWss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pVXaxl0fGFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
        "true_values = []\n",
        "for input, label in validation_loader:\n",
        "    true_values.extend(label)"
      ],
      "metadata": {
        "id": "CoEwbl4-H_wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array(result)\n",
        "val = np.array(true_values)\n",
        "accuracy = accuracy_score(val, pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(val, pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "Vuhr6XxpIB5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f020917-c61f-4fe7-ab55-860184a9c6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n",
            "Confusion Matrix:\n",
            "[[355   9  17   0   5]\n",
            " [ 65 175  64   0   2]\n",
            " [ 28  14 153   0   0]\n",
            " [  5   7   1  46   2]\n",
            " [ 42  12   9   4 269]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/validation/all/*\n",
        "!rm /content/validation/Earthquake/*\n",
        "!rm /content/validation/Flood/*\n",
        "!rm /content/validation/Hurricane/*\n",
        "!rm /content/validation/Landslides/*\n",
        "!rm /content/validation/Wildfire/*\n",
        "!rmdir /content/validation/all\n",
        "!rmdir /content/validation/Earthquake\n",
        "!rmdir /content/validation/Flood\n",
        "!rmdir /content/validation/Hurricane\n",
        "!rmdir /content/validation/Landslides\n",
        "!rmdir /content/validation/Wildfire\n",
        "!rm /content/validation/all_paired/*"
      ],
      "metadata": {
        "id": "vGUYftdqbWz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df69c6a-5ba5-4ea3-8056-23d09bcf0dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/validation/all/*': No such file or directory\n",
            "rmdir: failed to remove '/content/validation/all': No such file or directory\n",
            "rm: cannot remove '/content/validation/all_paired/*': No such file or directory\n"
          ]
        }
      ]
    }
  ]
}